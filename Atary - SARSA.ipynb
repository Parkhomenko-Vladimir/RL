{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import os\n",
    "import datetime\n",
    "from statistics import mean\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "ddd=5\n",
    "# len(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sarsa import SARSA\n",
    "Agent = SARSA(input_dimention = (4), num_actions = env.action_space.n,\n",
    "            gamma = 0.99, max_experiences = 1000, min_experiences = 100,\n",
    "            batch_size = 32,\n",
    "            lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, Agent, epsilon):\n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    losses = list()\n",
    "    \n",
    "    prev_observations = env.reset()\n",
    "    \n",
    "    prev_action = Agent.get_action(prev_observations, epsilon)\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "#         prev_observations = observations\n",
    "        n_observations, reward, done, _ = env.step(prev_action)\n",
    "        n_action = Agent.get_action(n_observations, epsilon)\n",
    "\n",
    "        exp = {'s': prev_observations, 'a': prev_action, 'r': reward, 's2': n_observations,  'a2': n_action,'done': done}\n",
    "        Agent.add_experience(exp)\n",
    "        loss = Agent.train_pop()\n",
    "        \n",
    "        rewards += reward\n",
    "        \n",
    "        prev_observations = n_observations\n",
    "        prev_action = n_action\n",
    "        \n",
    "        if done:\n",
    "            reward = -200\n",
    "            prev_observations = env.reset()\n",
    "            prev_action = Agent.get_action(prev_observations, epsilon)\n",
    "        \n",
    "        if isinstance(loss, int):\n",
    "            losses.append(loss)\n",
    "        else:\n",
    "            losses.append(loss.numpy())\n",
    "        iter += 1\n",
    "\n",
    "    return rewards, mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_video(env, Agent):\n",
    "    env = wrappers.Monitor(env, os.path.join(os.getcwd(), \"Sarsa_video\"), force=True)\n",
    "    rewards = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = Agent.get_action(observation, 0)\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        steps += 1\n",
    "        rewards += reward\n",
    "    print(\"Testing steps: {} rewards {}: \".format(steps, rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 episode reward: 47.0 eps: 0.99 avg reward (last 100): 47.0 episode loss:  0\n",
      "episode: 100 episode reward: 9.0 eps: 0.36237201786049694 avg reward (last 100): 14.623762376237623 episode loss:  0.12274975\n",
      "episode: 200 episode reward: 10.0 eps: 0.13263987810938213 avg reward (last 100): 10.752475247524753 episode loss:  0\n",
      "episode: 300 episode reward: 8.0 eps: 0.0485504851305729 avg reward (last 100): 9.732673267326733 episode loss:  0.51424694\n",
      "episode: 400 episode reward: 8.0 eps: 0.017771047742294682 avg reward (last 100): 9.455445544554456 episode loss:  0\n",
      "episode: 500 episode reward: 9.0 eps: 0.01 avg reward (last 100): 9.613861386138614 episode loss:  0\n",
      "episode: 600 episode reward: 12.0 eps: 0.01 avg reward (last 100): 10.158415841584159 episode loss:  0\n",
      "episode: 700 episode reward: 11.0 eps: 0.01 avg reward (last 100): 10.257425742574258 episode loss:  0\n",
      "episode: 800 episode reward: 13.0 eps: 0.01 avg reward (last 100): 12.257425742574258 episode loss:  0\n",
      "episode: 900 episode reward: 11.0 eps: 0.01 avg reward (last 100): 12.366336633663366 episode loss:  0\n",
      "episode: 1000 episode reward: 13.0 eps: 0.01 avg reward (last 100): 14.792079207920793 episode loss:  0\n",
      "episode: 1100 episode reward: 44.0 eps: 0.01 avg reward (last 100): 23.445544554455445 episode loss:  0.37974226\n",
      "episode: 1200 episode reward: 22.0 eps: 0.01 avg reward (last 100): 19.26732673267327 episode loss:  0\n",
      "episode: 1300 episode reward: 31.0 eps: 0.01 avg reward (last 100): 23.059405940594058 episode loss:  0.3042417\n",
      "episode: 1400 episode reward: 100.0 eps: 0.01 avg reward (last 100): 28.96039603960396 episode loss:  0.23604828\n",
      "episode: 1500 episode reward: 20.0 eps: 0.01 avg reward (last 100): 38.18811881188119 episode loss:  0\n",
      "episode: 1600 episode reward: 84.0 eps: 0.01 avg reward (last 100): 43.24752475247525 episode loss:  0.016015926\n",
      "episode: 1700 episode reward: 84.0 eps: 0.01 avg reward (last 100): 85.99009900990099 episode loss:  1.132055\n",
      "episode: 1800 episode reward: 200.0 eps: 0.01 avg reward (last 100): 115.44554455445545 episode loss:  0.42022526\n"
     ]
    }
   ],
   "source": [
    "# current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# log_dir = 'logs/Sarsa/' + current_time\n",
    "# summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "ep=[]\n",
    "ep_r=[]\n",
    "eps=[]\n",
    "avg_r=[]\n",
    "ep_l=[]\n",
    "\n",
    "N = 5000\n",
    "total_rewards = np.empty(N)\n",
    "epsilon = 1.\n",
    "decay = 0.99\n",
    "min_epsilon = 0.01\n",
    "for n in range(N):\n",
    "    epsilon = max(min_epsilon, epsilon * decay)\n",
    "    total_reward, losses = play_game(env, Agent, epsilon)\n",
    "    total_rewards[n] = total_reward\n",
    "    avg_rewards = total_rewards[max(0, n - 100):(n + 1)].mean()\n",
    "   \n",
    "#     with summary_writer.as_default():\n",
    "#         tf.summary.scalar('episode reward', total_reward, step=n)\n",
    "#         tf.summary.scalar('running avg reward(100)', avg_rewards, step=n)\n",
    "#         tf.summary.scalar('average loss)', losses, step=n)\n",
    "    if n % 100 == 0:\n",
    "        \n",
    "        ep.append(n)\n",
    "        ep_r.append(total_reward)\n",
    "        eps.append(epsilon)\n",
    "        avg_r.append(avg_rewards)\n",
    "        ep_l.append(losses)\n",
    "        \n",
    "        print(\"episode:\", n, \"episode reward:\", total_reward, \"eps:\", epsilon, \"avg reward (last 100):\", avg_rewards,\n",
    "              \"episode loss: \", losses)\n",
    "print(\"avg reward for last 100 episodes:\", avg_rewards)\n",
    "make_video(env, Agent)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = np.asarray(\n",
    "#          [ep,\n",
    "#          ep_r,\n",
    "#          eps,\n",
    "#          avg_r,\n",
    "#          ep_l])\n",
    "# np.savetxt('resalt_SARSA', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
