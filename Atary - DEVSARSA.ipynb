{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import os\n",
    "import datetime\n",
    "from statistics import mean\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "ddd=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EVSarsa import EVSARSA\n",
    "Agent = EVSARSA(input_dimention = (4), num_actions = env.action_space.n,\n",
    "            gamma = 0.99, max_experiences = 1000, min_experiences = 100,\n",
    "            batch_size = 32,\n",
    "            lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, Agent, epsilon):\n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    \n",
    "    observations = env.reset()\n",
    "    observations=np.around(observations, decimals = ddd)\n",
    "    \n",
    "    losses = list()\n",
    "    while not done:\n",
    "        action = Agent.get_action(observations, epsilon)\n",
    "        prev_observations = observations\n",
    "        \n",
    "        observations, reward, done, _ = env.step(action)\n",
    "        observations=np.around(observations, decimals = ddd)\n",
    "        \n",
    "        rewards += reward\n",
    "        if done:\n",
    "            reward = -200\n",
    "            env.reset()\n",
    "\n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
    "        Agent.add_experience(exp)\n",
    "        loss = Agent.train(epsilon)\n",
    "        \n",
    "        if isinstance(loss, int):\n",
    "            losses.append(loss)\n",
    "        else:\n",
    "            losses.append(loss.numpy())\n",
    "        iter += 1\n",
    "\n",
    "    return rewards, mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_video(env, Agent):\n",
    "    env = wrappers.Monitor(env, os.path.join(os.getcwd(), \"EVSARSA_video\"), force=True)\n",
    "    rewards = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = Agent.get_action(observation, 0)\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        steps += 1\n",
    "        rewards += reward\n",
    "    print(\"Testing steps: {} rewards {}: \".format(steps, rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 episode reward: 15.0 eps: 0.999 avg reward (last 100): 15.0 episode loss:  0\n",
      "episode: 100 episode reward: 23.0 eps: 0.9038873549665959 avg reward (last 100): 20.386138613861387 episode loss:  207.65202\n",
      "episode: 200 episode reward: 16.0 eps: 0.8178301806491574 avg reward (last 100): 22.06930693069307 episode loss:  125.77359\n",
      "episode: 300 episode reward: 26.0 eps: 0.7399663251239436 avg reward (last 100): 20.059405940594058 episode loss:  137.53578\n",
      "episode: 400 episode reward: 17.0 eps: 0.6695157201007336 avg reward (last 100): 19.584158415841586 episode loss:  116.956696\n",
      "episode: 500 episode reward: 18.0 eps: 0.6057725659163237 avg reward (last 100): 18.465346534653467 episode loss:  126.966705\n",
      "episode: 600 episode reward: 15.0 eps: 0.548098260578011 avg reward (last 100): 16.217821782178216 episode loss:  129.6257\n",
      "episode: 700 episode reward: 11.0 eps: 0.4959150020176678 avg reward (last 100): 16.07920792079208 episode loss:  101.774445\n",
      "episode: 800 episode reward: 12.0 eps: 0.44869999946146477 avg reward (last 100): 14.782178217821782 episode loss:  87.56683\n",
      "episode: 900 episode reward: 8.0 eps: 0.4059802359226587 avg reward (last 100): 13.504950495049505 episode loss:  112.47238\n",
      "episode: 1000 episode reward: 10.0 eps: 0.36732772934619257 avg reward (last 100): 13.069306930693068 episode loss:  86.61242\n",
      "episode: 1100 episode reward: 10.0 eps: 0.33235524492954527 avg reward (last 100): 12.386138613861386 episode loss:  120.29489\n",
      "episode: 1200 episode reward: 29.0 eps: 0.3007124156643058 avg reward (last 100): 17.386138613861387 episode loss:  147.98283\n",
      "episode: 1300 episode reward: 141.0 eps: 0.2720822322326576 avg reward (last 100): 77.97029702970298 episode loss:  128.02928\n"
     ]
    }
   ],
   "source": [
    "# current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# log_dir = 'logs/EVSARSA/' + current_time\n",
    "# summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "ep=[]\n",
    "ep_r=[]\n",
    "eps=[]\n",
    "avg_r=[]\n",
    "ep_l=[]\n",
    "\n",
    "N = 5000\n",
    "total_rewards = np.empty(N)\n",
    "epsilon = 1.\n",
    "decay = 0.999\n",
    "min_epsilon = 0.01\n",
    "for n in range(N):\n",
    "    epsilon = max(min_epsilon, epsilon * decay)\n",
    "    total_reward, losses = play_game(env, Agent, epsilon)\n",
    "    total_rewards[n] = total_reward\n",
    "    avg_rewards = total_rewards[max(0, n - 100):(n + 1)].mean()\n",
    "   \n",
    "#     with summary_writer.as_default():\n",
    "#         tf.summary.scalar('episode reward', total_reward, step=n)\n",
    "#         tf.summary.scalar('running avg reward(100)', avg_rewards, step=n)\n",
    "#         tf.summary.scalar('average loss)', losses, step=n)\n",
    "    if n % 100 == 0:\n",
    "        \n",
    "        ep.append(n)\n",
    "        ep_r.append(total_reward)\n",
    "        eps.append(epsilon)\n",
    "        avg_r.append(avg_rewards)\n",
    "        ep_l.append(losses)\n",
    "        \n",
    "        print(\"episode:\", n, \"episode reward:\", total_reward, \"eps:\", epsilon, \"avg reward (last 100):\", avg_rewards,\n",
    "              \"episode loss: \", losses)\n",
    "print(\"avg reward for last 100 episodes:\", avg_rewards)\n",
    "make_video(env, Agent)\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.asarray(\n",
    "         [ep,\n",
    "         ep_r,\n",
    "         eps,\n",
    "         avg_r,\n",
    "         ep_l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('resalt_EVsarasa', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.loadtxt('resalt_EVsarasa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
