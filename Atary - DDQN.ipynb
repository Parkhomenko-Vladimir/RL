{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import os\n",
    "import datetime\n",
    "from statistics import mean\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "ddd=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DDQN import DDQN\n",
    "Agent = DDQN(input_dimention = (4), num_actions = env.action_space.n,\n",
    "            gamma = 0.99, max_experiences = 1000, min_experiences = 100,\n",
    "            batch_size = 32,\n",
    "             lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, Agent, epsilon):\n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    \n",
    "    observations = env.reset()\n",
    "    observations=np.around(observations, decimals = ddd)\n",
    "    \n",
    "    losses = list()\n",
    "    while not done:\n",
    "        action = Agent.get_action(observations, epsilon)\n",
    "        prev_observations = observations\n",
    "        \n",
    "        observations, reward, done, _ = env.step(action)\n",
    "        observations=np.around(observations, decimals = ddd)\n",
    "        \n",
    "        rewards += reward\n",
    "        if done:\n",
    "            reward = -200\n",
    "            env.reset()\n",
    "\n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
    "        Agent.add_experience(exp)\n",
    "        loss = Agent.train()\n",
    "        \n",
    "        if isinstance(loss, int):\n",
    "            losses.append(loss)\n",
    "        else:\n",
    "            losses.append(loss.numpy())\n",
    "        iter += 1\n",
    "\n",
    "    return rewards, mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_video(env, Agent):\n",
    "    env = wrappers.Monitor(env, os.path.join(os.getcwd(), \"DDQN_video\"), force=True)\n",
    "    rewards = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = Agent.get_action(observation, 0)\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        steps += 1\n",
    "        rewards += reward\n",
    "    print(\"Testing steps: {} rewards {}: \".format(steps, rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 episode reward: 64.0 eps: 0.999 avg reward (last 100): 64.0 episode loss:  0\n",
      "episode: 100 episode reward: 8.0 eps: 0.9038873549665959 avg reward (last 100): 21.257425742574256 episode loss:  177.25899\n",
      "episode: 200 episode reward: 15.0 eps: 0.8178301806491574 avg reward (last 100): 22.603960396039604 episode loss:  134.64125\n",
      "episode: 300 episode reward: 19.0 eps: 0.7399663251239436 avg reward (last 100): 21.445544554455445 episode loss:  227.00496\n"
     ]
    }
   ],
   "source": [
    "# current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# log_dir = 'logs/DDQN/' + current_time\n",
    "# summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "ep=[]\n",
    "ep_r=[]\n",
    "eps=[]\n",
    "avg_r=[]\n",
    "ep_l=[]\n",
    "\n",
    "N = 5000\n",
    "total_rewards = np.empty(N)\n",
    "epsilon = 1.\n",
    "decay = 0.999\n",
    "min_epsilon = 0.01\n",
    "for n in range(N):\n",
    "    epsilon = max(min_epsilon, epsilon * decay)\n",
    "    total_reward, losses = play_game(env, Agent, epsilon)\n",
    "    total_rewards[n] = total_reward\n",
    "    avg_rewards = total_rewards[max(0, n - 100):(n + 1)].mean()\n",
    "   \n",
    "#     with summary_writer.as_default():\n",
    "#         tf.summary.scalar('episode reward', total_reward, step=n)\n",
    "#         tf.summary.scalar('running avg reward(100)', avg_rewards, step=n)\n",
    "#         tf.summary.scalar('average loss)', losses, step=n)\n",
    "    if n % 100 == 0:\n",
    "        \n",
    "        ep.append(n)\n",
    "        ep_r.append(total_reward)\n",
    "        eps.append(epsilon)\n",
    "        avg_r.append(avg_rewards)\n",
    "        ep_l.append(losses)\n",
    "        \n",
    "        print(\"episode:\", n, \"episode reward:\", total_reward, \"eps:\", epsilon, \"avg reward (last 100):\", avg_rewards,\n",
    "              \"episode loss: \", losses)\n",
    "print(\"avg reward for last 100 episodes:\", avg_rewards)\n",
    "make_video(env, Agent)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.asarray(\n",
    "         [ep,\n",
    "         ep_r,\n",
    "         eps,\n",
    "         avg_r,\n",
    "         ep_l])\n",
    "np.savetxt('resalt_DDQN', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
